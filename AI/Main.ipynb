{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk4lbEgL_W9V"
      },
      "outputs": [],
      "source": [
        "!pip install pymorphy2 \n",
        "!pip install pymorphy2-dicts\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y6M5Wgpk5xB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import json \n",
        "import io\n",
        "\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pymorphy2\n",
        "from tqdm import tqdm\n",
        "from catboost import CatBoostClassifier, Pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgCwe7sDlAvM"
      },
      "outputs": [],
      "source": [
        "with io.open(\"/content/drive/MyDrive/hacaton_ai2021spb/data3.json\", mode=\"r\", encoding=\"utf-8\") as file:\n",
        "    js = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihQi20yCoJXR"
      },
      "outputs": [],
      "source": [
        "all_data = []\n",
        "for k in js:\n",
        "  data = js[k]\n",
        "  company = data['company']\n",
        "  gos = company['goszakupki']\n",
        "  habr = company['habr']\n",
        "  vcru = company['vcru']\n",
        "  tj = company['tinkoff_journal']\n",
        "  people = data['people']\n",
        "  label = data['nomination'].replace('\\n', '').replace('   ', '')\n",
        "\n",
        "  texts_kw = habr[\"company_info\"][\"description\"] if habr[\"company_info\"] else '' + ' '  + ' '.join(habr[\"company_info\"][\"industries\"] if habr[\"company_info\"] else []) + ' '  + ' '.join(tj['texts']) + ' ' + ' '.join([x['text'] for x in company[\"cnews\"]]) + ' '.join([x['description'] for x in gos] if gos else []) \n",
        "\n",
        "  texts_pos_neg = []\n",
        "  for text in company['cnews']:\n",
        "    texts_pos_neg.append(text['text'])\n",
        "  for text in company['habr']['references']:\n",
        "    texts_pos_neg.append(text)\n",
        "  for text in tj['texts']:\n",
        "    texts_pos_neg.append(text)\n",
        "  for text in vcru['texts']:\n",
        "    texts_pos_neg.append(text)\n",
        "\n",
        "  people_texts = []\n",
        "  for person in people:\n",
        "    for text in person['cnews']:\n",
        "      people_texts.append(text['text'])\n",
        "    for text in person['habr']['references']:\n",
        "      people_texts.append(text)\n",
        "\n",
        "  all_data.append([k, texts_kw, texts_pos_neg, people_texts, label])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjNX5wLm0WGG"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data=all_data, columns=['company_name', 'text_for_keywords', 'text_for_classification', 'people', 'label'])\n",
        "df_company_kw = pd.concat([df['company_name'], df['text_for_keywords'], df['label']], axis=1)\n",
        "encode_company_classes = {x: i for (i,x) in enumerate(np.unique(list(df['label'])))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sEmBRl5-jV5"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer()\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
        "\n",
        "def get_kw(text, first=True):\n",
        "  sw = '''что из есть он том они сейчас быть который время например поэтому несколько раз тут а е и ж м о на не ни об но он мне мои мож она они оно мной много многочисленное многочисленная многочисленные многочисленный мною мой мог могут можно может можхо мор моя моё мочь над нее оба нам нем  нами ними мимо немного одной одного менее однажды однако меня нему меньше ней наверху него ниже мало надо один одиннадцать одиннадцатый назад наиболее недавно миллионов недалеко между низко меля нельзя нибудь непрерывно наконец никогда никуда нас наш нет нею неё нихмира наша наше наши ничего начала нередко несколько обычно опять около мы ну нх от отовсюду особенно нужно очень отсюда в во вон вниз внизу вокруг вот восемнадцать восемнадцатый восемь восьмой вверх вам вами важное важнаяважные важный  вдали везде ведь вас ваш ваша ваше ваши впрочем весь вдруг вы все второй всем всеми времени время всему всего всегда всех всею всю вся всё всюду г год говорил говорит года году где да ееза из ли же им до по ими под иногда довольно именно долго позже более должно пожалуйста значит иметь больше пока ему имя пор пора потом потому после почему почти посреди ей два две двенадцать двенадцатый двадцать двадцатый двух его дел или без день занят занята занято заняты действительно давно девятнадцать девятнадцатый девять девятый даже алло жизнь далеко близко здесь дальше для лет зато даром первый перед затем зачем лишь десять десятый ею её их бы еще прибыл про  процентов против просто бывает бывь если люди была были было  будем  будет будете будешь прекрасно буду будь будто будут ещё пятнадцать пятнадцатый друго другое другой другие другая других есть пять быть лучше пятый к ком конечно кому кого когда которой которого которая которые который которых кем каждое каждая каждые каждый кажется как какой какая кто кроме  куда кругом с  т у я та те уж со то том снова тому совсем того тогда тоже собой тобой собою тобою сначалатолько уметь  тот тою хорошо хотеть хочешь хоть хотя свое свои твой своей своего своих свою твоя твоё раз уже сам там тем чем сама сами теми само рано самом самому самой самого семнадцать семнадцатый самим самими самихсаму семь  чему раньше сейчас чего сегодня себе тебе сеаой человек разве теперь себя тебя седьмой спасибо слишком так такое такой такие также такая сих тех чаще четвертый через  часто шестой шестнадцать шестнадцатый шесть четыре четырнадцать четырнадцатый сколько сказал сказала сказать ту ты три эта эти что это чтоб этом этому этой этого чтобы этот стал туда этим этимирядом тринадцать  тринадцатый этих третий тут эту суть чуть тысяч'''.split()\n",
        "  def lemmatize(texts):\n",
        "      m = pymorphy2.MorphAnalyzer() \n",
        "      texts = [m.parse(x)[0].normal_form for x in texts]\n",
        "      return texts\n",
        "\n",
        "  def pre_process(text, sw):\n",
        "      \n",
        "      text=text.lower()\n",
        "      \n",
        "      text=re.sub(\"</?.*?>\",\" <> \",text)\n",
        "      \n",
        "      text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "      text_splitted = []\n",
        "      for word in text.split():\n",
        "        if word not in sw:\n",
        "          text_splitted.append(word)\n",
        "      lem = lemmatize(text_splitted)\n",
        "      text_splitted_n = []\n",
        "      for word in lem:\n",
        "        if word not in sw:\n",
        "          text_splitted_n.append(word)   \n",
        "      return text_splitted_n\n",
        "\n",
        "  text_n = pre_process(text, sw)\n",
        "  if first:\n",
        "    word_count_vector = cv.fit_transform(text_n)\n",
        "    tfidf_transformer.fit(word_count_vector)\n",
        "  else:\n",
        "    word_count_vector = cv.transform(text_n)\n",
        "\n",
        "  def sort_coo(coo_matrix):\n",
        "      tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "      return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "  def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "      \n",
        "      sorted_items = sorted_items[:topn]\n",
        "\n",
        "      score_vals = []\n",
        "      feature_vals = []\n",
        "\n",
        "      for idx, score in sorted_items:\n",
        "          fname = feature_names[idx]\n",
        "          \n",
        "          score_vals.append(round(score, 3))\n",
        "          feature_vals.append(feature_names[idx])\n",
        "\n",
        "      results= {}\n",
        "      for idx in range(len(feature_vals)):\n",
        "          results[feature_vals[idx]]=score_vals[idx]\n",
        "      \n",
        "      return results\n",
        "\n",
        "  def sort_coo(coo_matrix):\n",
        "      tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "      return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "  def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "      \n",
        "      sorted_items = sorted_items[:topn]\n",
        "\n",
        "      score_vals = []\n",
        "      feature_vals = []\n",
        "\n",
        "      for idx, score in sorted_items:\n",
        "          fname = feature_names[idx]\n",
        "          \n",
        "          score_vals.append(round(score, 3))\n",
        "          feature_vals.append(feature_names[idx])\n",
        "\n",
        "      results= {}\n",
        "      for idx in range(len(feature_vals)):\n",
        "          results[feature_vals[idx]]=score_vals[idx]\n",
        "      \n",
        "      return results\n",
        "  feature_names=cv.get_feature_names()\n",
        "\n",
        "  tf_idf_vector=tfidf_transformer.transform(cv.transform([' '.join(text_n)]))\n",
        "\n",
        "  sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "  keywords=extract_topn_from_vector(feature_names,sorted_items, 100)\n",
        "\n",
        "  return keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqAc7j1B_Q0J"
      },
      "outputs": [],
      "source": [
        "kw = []\n",
        "i = 0\n",
        "for data in tqdm(df_company_kw['text_for_keywords']):\n",
        "  if i == 0:\n",
        "    kw.append(get_kw(data, True))\n",
        "  else:\n",
        "    kw.append(get_kw(data, False))\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6TOhFG_AMCv"
      },
      "outputs": [],
      "source": [
        "kw_normal = []\n",
        "for keywords in kw:\n",
        "  kw_ = []\n",
        "  for k in keywords:\n",
        "    kw_.append(k)\n",
        "  kw_normal.append(kw_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NomnDHH8C6v7"
      },
      "outputs": [],
      "source": [
        "df_company_kw['kw'] = [' '.join(x) for x in kw_normal]\n",
        "df_company_kw['label'] = [encode_company_classes[x] for x in df_company_kw['label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQvS2mUCDaJo"
      },
      "outputs": [],
      "source": [
        "text_features = ['kw']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-aOF0OQD1Bd"
      },
      "outputs": [],
      "source": [
        "X = df_company_kw.drop(['text_for_keywords', 'company_name', 'label'], axis=1)\n",
        "y = df_company_kw['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRfPyisVFpuB"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y.values,test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5NdndlIF8_Q"
      },
      "outputs": [],
      "source": [
        "train_pool = Pool(\n",
        "    X_train, \n",
        "    y_train, \n",
        "    text_features=text_features, \n",
        "    feature_names=list(X_train)\n",
        ")\n",
        "valid_pool = Pool(\n",
        "    X_test, \n",
        "    y_test,\n",
        "    text_features=text_features, \n",
        "    feature_names=list(X_train)\n",
        ")\n",
        "\n",
        "catboost_params = {\n",
        "    'iterations': 3000,\n",
        "    'learning_rate': 0.01,\n",
        "    'eval_metric': 'Accuracy',\n",
        "    'early_stopping_rounds': 2000,\n",
        "    'use_best_model': True,\n",
        "    'verbose': 100\n",
        "}\n",
        "model = CatBoostClassifier(**catboost_params)\n",
        "model.fit(train_pool, eval_set=valid_pool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0quAjF8dGHmW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test,pred))\n",
        "print(accuracy_score(y_test,pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcK_2lf5TLU_"
      },
      "outputs": [],
      "source": [
        "model.save_model('model_classif', format=\"cbm\", export_parameters=None, pool=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeLYCqMvUWDR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
